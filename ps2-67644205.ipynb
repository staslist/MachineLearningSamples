{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and SVM\n",
    "\n",
    "CS184A/284A Assignment 2<br>\n",
    "Fall 2016\n",
    "\n",
    "\n",
    "In this assignment, we will walk you through the process of implementing\n",
    "- Logistic regression\n",
    "- Gradient descent \n",
    "- Subgr\n",
    "- Support Vector Machines (SVM)\n",
    "\n",
    "The purpose of this assignment is to familiarize you with basic knowledge about linear regression, including optimization and cross-validation, and help you gain proficiency in writing efficient code.\n",
    "\n",
    "** Please don't add or remove any code cells, as it might break our automatic grading system and affect your grade. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Honor Code:** I hereby agree to abide the UCI Honor Code and that of the Computer Science Department, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
    "\n",
    "**Signature**: *(Stanislav Listopad)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6)\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets, linear_model, cross_validation, metrics, preprocessing\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pima Indians diabetes dataset\n",
    "https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "diabetes_X = dataset[:,0:8]\n",
    "diabetes_Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rescale each feature to be mean 0 and var 1\n",
    "diabetes_X_scale = preprocessing.scale(diabetes_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a constant of 1 to the first column of a matrix\n",
    "def add_constant(x):\n",
    "    \"\"\" Add a column of constant 1 \"\"\"\n",
    "    x = np.concatenate( (np.ones((x.shape[0],1)), x), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diabetes_X_scale_aug = add_constant(diabetes_X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                            diabetes_X_scale_aug, diabetes_Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a score function to evaluate prediction accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score(y_pred, y_obs):\n",
    "    \"\"\"\n",
    "    y_obs:  vector of observed labels\n",
    "    y_pred: vector of predicted labels\n",
    "\n",
    "    returns the percentage of predicted labels matching the observed labels. \n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    score = 0\n",
    "    index = 0\n",
    "    size = y_pred.shape[0]\n",
    "    while(index < size):\n",
    "        if(y_pred[index] == y_obs[index]):\n",
    "            score +=1\n",
    "        index +=1 \n",
    "    score = score / float(size)\n",
    "\n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36796536796536794"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test this function\n",
    "print \"=== For autograder ===\"\n",
    "score(np.ones(y_test.shape), y_test) \n",
    "\n",
    "# should return  0.36796536796536794"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "* Model:    $$P(y=1|x) = g(\\theta^T x)$$ \n",
    "where $g(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function.\n",
    "    The model predicts y=1 if $P(y=1|x)>0.5$ and 0 otherwise.\n",
    "\n",
    "* Cost function:\n",
    "$$J(\\theta) = - \\sum_{i=1}^m [y^{(i)} \\log(P(y^{(i)}=1)|x^{(i)})+(1-y^{(i)}) \\log(P(y^{(i)}=0)|x^{(i)})]$$\n",
    "* In maxtir representation: let $y$ be an $m$-dimensional vector and $X$ be an $m$ by $n+1$ matrix.\n",
    "$$J(\\theta) = - \\frac{1}{m}[y^T \\log g(X\\theta) + (1-y)^T \\log (1- g(X\\theta))]$$\n",
    "\n",
    "* The gradient of $J(\\theta)$ w.r.t. $\\theta$ is\n",
    "$$\\nabla J(\\theta) = \\frac{1}{m} X^T(g(X\\theta)-y)$$\n",
    "\n",
    "** Now define a function which receives $X$, $y$, $\\theta$ as input and outputs both cost and gradient** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[ 0.26894142  0.5         0.73105858]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    f = np.exp(-x)\n",
    "    f = np.add(f, 1)\n",
    "    f = np.divide(1, f)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Test this function\n",
    "print \"=== For autograder ===\"\n",
    "print sigmoid(np.array([-1., 0, 1.]))\n",
    "# should produce [ 0.26894142  0.5         0.73105858]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gradient descent\n",
    "\n",
    "Next we walk through steps of implementing gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "(0.97992835418488955, array([-1.12878382, -1.52650907]))\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression_cost_grad(X, y, theta):\n",
    "    \"\"\"\n",
    "    calculate cost and grad of logistic regression cost function\n",
    "    X:  m x (n+1)  (m: number of samples, n: number of feature dimension)\n",
    "    y:  m x 1  (target)\n",
    "    beta: (n+1),   (beta_0 is the intercept)\n",
    "    output: cost, grad   where grad should be an one-dimensional array with size n+1. \n",
    "    \"\"\"    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x_1 = sigmoid(np.dot(X, theta))\n",
    "    m = X.shape[0]\n",
    "    x_2 = np.dot(np.transpose(y), np.log(x_1))\n",
    "    x_3 = np.dot(np.transpose(np.subtract(1, y)), np.log(np.subtract(1, x_1)))\n",
    "    cost = np.dot((-1.0/m), (x_2 + x_3))\n",
    "    \n",
    "    x_2 = np.subtract(x_1, y)\n",
    "    x_3 = np.dot(np.transpose(X), x_2)\n",
    "    grad = np.dot((1.0/m), x_3)\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "# Test this function\n",
    "print \"=== For autograder ===\"\n",
    "print logistic_regression_cost_grad(np.array([[1.,2.],[2, 3],[3,4]]), np.array([0,1,1]), np.array([1,-1]))  \n",
    "# should produce (0.97992835418488955, array([-1.12878382, -1.52650907]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(f, x0, step, iterations, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the stochastic gradient descent method in this        #\n",
    "    # function.                                                       #\n",
    "    # Inputs:                                                         #\n",
    "    #   - f: the function to optimize, it should take a single        #\n",
    "    #        argument and yield two outputs, a cost and the gradient  #\n",
    "    #        with respect to the arguments                            #\n",
    "    #   - x0: the initial point to start SGD from                     #\n",
    "    #   - step: the step size for SGD                                 #\n",
    "    #   - iterations: total iterations to run SGD for                 #\n",
    "    #   - PRINT_EVERY: specifies every how many iterations to output  #\n",
    "    # Output:                                                         #\n",
    "    #   - x: the parameter value after SGD finishes                   #\n",
    "    ###################################################################\n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    #\n",
    "    ANNEAL_EVERY = 10000    \n",
    "\n",
    "    x = x0    \n",
    "    for iter in xrange(iterations):\n",
    "\n",
    "        ### YOUR CODE HERE        \n",
    "        cost, grad = f(x)\n",
    "        x -= step*grad                \n",
    "        ### END YOUR CODE      \n",
    "        \n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            print \"iter: %d cost: %f\" % (iter, cost)               \n",
    "      \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "            \n",
    "        #plt.plot(iter, cost, '.b')            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch gradient descent on the diabetes training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up parameters for running sgd,  do not change these parameters\n",
    "np.random.seed(1)\n",
    "theta0 = np.random.randn(X_train.shape[1],)  # randomly initialize theta\n",
    "step = 0.1     # step size\n",
    "niter = 5000   # number of iterations\n",
    "\n",
    "## Now run gradient descent on the diabetes training data using linear_regression_cost_grad\n",
    "\n",
    "### YOUR CODE HERE   \n",
    "index = 0\n",
    "while (index < niter):\n",
    "    cost, grad = logistic_regression_cost_grad(X_train, y_train, theta0)\n",
    "    theta0 = np.subtract(theta0, np.multiply(step, grad))\n",
    "    index += 1\n",
    "theta_batch = theta0\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "0.776536312849 0.78354978355\n"
     ]
    }
   ],
   "source": [
    "# calcluate prediction accuracy of the trained logistic regression model\n",
    "\n",
    "### YOUR CODE HERE\n",
    "y_pred = np.zeros(y_train.shape[0])\n",
    "\n",
    "index = 0\n",
    "while(index < X_train.shape[0]):\n",
    "    if(sigmoid(np.dot(np.transpose(theta_batch), X_train[index, :])) > 0.5):\n",
    "        y_pred[index] = 1\n",
    "    else:\n",
    "        y_pred[index] = 0\n",
    "    index += 1\n",
    "train_accuracy = score(y_pred, y_train)\n",
    "\n",
    "\n",
    "index = 0\n",
    "y_pred = np.zeros(y_test.shape[0])\n",
    "\n",
    "while(index < X_test.shape[0]):\n",
    "    if(sigmoid(np.dot(np.transpose(theta_batch), X_test[index, :])) > 0.5):\n",
    "        y_pred[index] = 1\n",
    "    else:\n",
    "        y_pred[index] = 0\n",
    "    index += 1\n",
    "test_accuracy = score(y_pred, y_test)\n",
    "### END OF YOUR CODE\n",
    "\n",
    "\n",
    "### END OF YOUR CODE\n",
    "\n",
    "print \"=== For autograder ===\"\n",
    "print train_accuracy, test_accuracy\n",
    "# should retrun 0.776536312849 0.78354978355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM \n",
    "\n",
    "- Linear classifier:  Assign $y=+1$ if $w^Tx+b>0$ and $y=-1$ otherwise. $w\\in R^n$ and $b\\in R$ are parameters of the model.\n",
    "\n",
    "- Hinge loss function: $$ J(w,b) = \\sum_{i=1}^m \\max\\{0, 1- y^{(i)}(w^Tx^{(i)}+b)\\} + \\frac{\\alpha}{2} \\|w\\|_2^2 $$\n",
    "where $\\alpha$ is the regularization parameter\n",
    "- In this assignment, we will implement a subgradient method to solve SVM optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change data according to SVM format\n",
    "X_train_svm = X_train[:,1:]   # excluding constant \n",
    "y_train_svm = y_train \n",
    "y_train_svm[y_train==0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random initialization of w and b\n",
    "np.random.seed(1)\n",
    "theta0 = np.random.randn(X_train.shape[1],)  # randomly initialize theta\n",
    "w = theta0[1:]\n",
    "b = theta0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1278.3956978213123,\n",
       " array([ 223.        , -102.90084914, -208.88830549,  -46.54214938,\n",
       "         -42.45648627, -163.36303984,  -47.83554692, -105.40353328,\n",
       "         -82.36221523]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now implement a function that compute the cost and sub-gradient of the SVM cost function\n",
    "def SVM_cost_grad(X, y, w, b, alpha):\n",
    "    \"\"\"\n",
    "    calculate cost and grad of mean square error cost function\n",
    "    X:  (m,n)  (m: number of samples, n: number of feature dimension)\n",
    "    y:  (m,)  target (+1, -1)\n",
    "    w:  (n,)\n",
    "    b:  scalar \n",
    "    alpha: scalar\n",
    "    output: cost, grad\n",
    "            grad: one-dimensional array with shape (n+1,), gradidents w.r.t. w and b\n",
    "            with the FIRST component being gradient w.r.t. b\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE    \n",
    "\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    index = 0\n",
    "    sum = 0\n",
    "    while(index < m):\n",
    "        x_1 = np.dot(np.transpose(w), X[index, :])\n",
    "        x_2 = np.add(x_1, b)\n",
    "        x_3 = np.dot(y[index], x_2)\n",
    "        x_4 = np.subtract(1, x_3)\n",
    "        x_5 = np.maximum(0, x_4)\n",
    "        sum += x_5\n",
    "        index += 1\n",
    "    cost = sum + np.dot((alpha/2.0), np.linalg.norm(np.square(w), 1))\n",
    "    \n",
    "    index = 0\n",
    "    sumW = np.zeros(n)\n",
    "    sumb = 0\n",
    "    \n",
    "    grad = np.zeros(n+1)\n",
    "    \n",
    "    while(index < m):\n",
    "        x_1 = np.dot(np.transpose(w), X[index, :])\n",
    "        x_2 = np.add(x_1, b)\n",
    "        x_3 = np.dot(y[index], x_2)\n",
    "        if(x_3 < 1.0):\n",
    "            x_1 = np.dot(-1, y[index])\n",
    "            x_2 = np.dot(x_1, X[index, :])\n",
    "            sumW = np.add(sumW, x_2)\n",
    "            sumb += x_1\n",
    "        index += 1\n",
    "    sumW = np.add(sumW, np.dot(alpha, w))\n",
    "    grad[1:(n+1)] = sumW\n",
    "    grad[0] = sumb\n",
    "        \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "# Test this function\n",
    "print \"=== For autograder ===\"\n",
    "\n",
    "SVM_cost_grad(X_train_svm,y_train_svm, w, b, 1.0)\n",
    "# (1278.3956978213123, array([ 223.        , -102.90084914, -208.88830549,  -46.54214938,\n",
    "#         -42.45648627, -163.36303984,  -47.83554692, -105.40353328,\n",
    "#         -82.36221523]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up parameters for running sgd,  do not change these parameters\n",
    "alpha = 10\n",
    "step = 0.001    # step size\n",
    "niter = 1000   # number of iterations\n",
    "\n",
    "## Using subgradient method to find optimal paramters of linear SVM\n",
    "## Now run gradient descent on the diabetes training data using SVM_cost_grad\n",
    "\n",
    "### YOUR CODE HERE   \n",
    "index = 0\n",
    "while (index < niter):\n",
    "    cost, grad = SVM_cost_grad(X_train_svm, y_train_svm, w, b, alpha)\n",
    "    theta0 = np.subtract(theta0, np.multiply(step, grad))\n",
    "    w = theta0[1:]\n",
    "    b = theta0[0]\n",
    "    index += 1\n",
    "theta_svm = theta0\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[-0.70565464  0.24119426  0.8316116  -0.20231241 -0.11929713 -0.0271685\n",
      "  0.54491416  0.16806344  0.09385541]\n"
     ]
    }
   ],
   "source": [
    "print \"=== For autograder ===\"\n",
    "print theta_svm\n",
    "# should produce [-0.70665464  0.24140978  0.83001714 -0.20227141 -0.12249842 -0.03130514\n",
    "#  0.54549111  0.16816575  0.09153396]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "0.778398510242 0.792207792208\n"
     ]
    }
   ],
   "source": [
    "# calcluate prediction accuracy of the trained SVM model\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "#theta_svm = [-0.70665464,  0.24140978,  0.83001714, -0.20227141, -0.12249842, -0.03130514, 0.54549111,  0.16816575,  0.09153396]\n",
    "\n",
    "y_svm_pred = np.zeros(y_train_svm.shape[0])\n",
    "\n",
    "index = 0\n",
    "while(index < X_train_svm.shape[0]):\n",
    "    if(sigmoid(np.add(theta_svm[0], np.dot(np.transpose(theta_svm[1::]), X_train_svm[index, :]))) > 0.5):\n",
    "        y_svm_pred[index] = 1\n",
    "    else:\n",
    "        y_svm_pred[index] = -1\n",
    "    index += 1\n",
    "train_accuracy_svm = score(y_svm_pred, y_train_svm)\n",
    "\n",
    "\n",
    "# change data according to SVM format\n",
    "X_test_svm = X_test[:,1:]   # excluding constant \n",
    "y_test_svm = y_test \n",
    "y_test_svm[y_test==0] = -1\n",
    "\n",
    "index = 0\n",
    "y_svm_pred = np.zeros(y_test_svm.shape[0])\n",
    "\n",
    "while(index < X_test_svm.shape[0]):\n",
    "    if(sigmoid(np.add(theta_svm[0], np.dot(np.transpose(theta_svm[1::]), X_test_svm[index, :]))) > 0.5):\n",
    "        y_svm_pred[index] = 1\n",
    "    else:\n",
    "        y_svm_pred[index] = -1\n",
    "    index += 1\n",
    "test_accuracy_svm = score(y_svm_pred, y_test_svm)\n",
    "### END OF YOUR CODE\n",
    "\n",
    "print \"=== For autograder ===\"\n",
    "print train_accuracy_svm, test_accuracy_svm\n",
    "# should retrun 0.780260707635 0.792207792208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
