{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "CS184A/284A Assignment 3<br>\n",
    "Fall 2016\n",
    "\n",
    "\n",
    "In this assignment, we will walk you through the process of implementing\n",
    "- Neural networks\n",
    "- Gradient checking\n",
    "- Stochastic gradient descent \n",
    "\n",
    "\n",
    "The purpose of this assignment is to familiarize you with basic knowledge about neural networks, including forward, backward propagation and gradient checking, and help you gain proficiency in writing efficient code.\n",
    "\n",
    "** Please don't add or remove any code cells, as it might break our automatic grading system and affect your grade. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Honor Code:** I hereby agree to abide the UCI Honor Code and that of the Computer Science Department, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
    "\n",
    "**Signature**: *(Stanislav Listopad)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg',warn=True,force=True)\n",
    "matplotlib.get_backend()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Softmax\n",
    "\n",
    "Given an input matrix of N rows and d columns, compute the softmax prediction for each row. That is, when the input is\n",
    "\n",
    "    [[1,2],\n",
    "    [3,4]]\n",
    "\n",
    "the output of your functions should be\n",
    "\n",
    "    [[0.2689, 0.7311],\n",
    "    [0.2689, 0.7311]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" Softmax function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.                             #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    #if it is a matrix:\n",
    "    y = np.empty(x.shape)\n",
    "    z = np.empty(y.shape)\n",
    "    indexRows = 0\n",
    "    #if it is a matrix:\n",
    "    if (x.shape[0] != x.size):\n",
    "        for row in x:\n",
    "            normRow = np.subtract(row, np.mean(row))\n",
    "            #print normRow\n",
    "            y[indexRows] = np.exp(normRow)\n",
    "            z[indexRows] = np.sum(y[indexRows])\n",
    "            indexRows += 1\n",
    "    #if it is a vector\n",
    "    else:\n",
    "        y = np.exp(np.subtract(x, np.mean(x)))\n",
    "        z = np.sum(y)\n",
    "\n",
    "        \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return y/z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "[ 0.26894142  0.73105858]\n"
     ]
    }
   ],
   "source": [
    "# Verify your softmax implementation\n",
    "\n",
    "print (\"=== For autograder ===\")\n",
    "print (softmax(np.array([[1001,1002],[3,4]])))\n",
    "print (softmax(np.array([[-1001,-1002]])))\n",
    "print (softmax(np.array([1,2])))\n",
    "\n",
    "# should produce\n",
    "#[[ 0.26894142  0.73105858]\n",
    "# [ 0.26894142  0.73105858]]\n",
    "#[[ 0.73105858  0.26894142]]\n",
    "#[ 0.26894142  0.73105858]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural network basics\n",
    "\n",
    "In this part, you're going to implement\n",
    "\n",
    "* A sigmoid activation function and its gradient\n",
    "* A forward propagation for a simple neural network with cross-entropy cost\n",
    "* A backward propagation algorithm to compute gradients for the parameters\n",
    "* Gradient / derivative check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return (1/(1 + np.exp(-x)))\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "  \n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\" Sigmoid gradient function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return (f * (1-f))\n",
    "\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "# Check your sigmoid implementation\n",
    "x = np.array([[1, 2], [-1, -2]])\n",
    "f = sigmoid(x)\n",
    "g = sigmoid_grad(f)\n",
    "print (\"=== For autograder ===\")\n",
    "print (f)\n",
    "print (g)\n",
    "\n",
    "# should produce\n",
    "# [[ 0.73105858  0.88079708]\n",
    "#  [ 0.26894142  0.11920292]]\n",
    "# [[ 0.19661193  0.10499359]\n",
    "#  [ 0.19661193  0.10499359]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "    \n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "    \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        random.setstate(rndstate)\n",
    "        x[ix] += h\n",
    "        fxhp, g = f(x)\n",
    "        x[ix] -= (2*h)\n",
    "        fxhn, g = f(x)\n",
    "        numgrad = (fxhp - fxhn)/(2*h)\n",
    "        \n",
    "        #restore x[ix] back to original value\n",
    "        x[ix] += h\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print (\"Gradient check failed.\")\n",
    "            print (\"First gradient error found at index %s\" % str(ix))\n",
    "            print (\"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print (\"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for the gradient checker\n",
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "print (\"=== For autograder ===\")\n",
    "gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up fake data and parameters for the neural network\n",
    "np.random.seed(1)\n",
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "def xrange(x):\n",
    "    return iter(range(x))\n",
    "for i in xrange(N):\n",
    "    labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_predict(data, params, dimensions):\n",
    "    \"\"\" Forward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    # dimension: [n1, n2 , n3]  - n1: input, n2: hidden, n3: output \n",
    "    # params: flattened weights and biases\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t:t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1]))\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t:t+dimensions[1]], (1, dimensions[1]))\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t:t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t:t+dimensions[2]], (1, dimensions[2]))\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    # forward propagation\n",
    "    a_1 = data\n",
    "    \n",
    "    z_2 = np.add(np.dot(a_1, W1), b1)\n",
    "    a_2 = sigmoid(z_2)\n",
    "    z_3 = np.add(np.dot(a_2, W2), b2)\n",
    "    a_3 = softmax(z_3)\n",
    "    return a_3\n",
    "    \n",
    "\n",
    "\n",
    "    ### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25448924  0.01311372  0.0335646   0.01429178  0.01335743  0.08147074\n",
      "   0.09045286  0.30615653  0.0216449   0.1714582 ]]\n"
     ]
    }
   ],
   "source": [
    "# Check your implementation\n",
    "#print data[0,:]\n",
    "print (nn_predict(data[0,:], params, dimensions))\n",
    "\n",
    "# should produce\n",
    "# [[ 0.25448924 0.01311372 0.0335646 0.01429178 0.01335743 0.08147074\n",
    "#    0.09045286 0.30615653 0.0216449 0.1714582 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params):\n",
    "    \"\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    # dimension: [n1, n2 , n3]  - n1: input, n2: hidden, n3: output \n",
    "    # params: flattened weights and biases\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t:t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1]))\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t:t+dimensions[1]], (1, dimensions[1]))\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t:t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t:t+dimensions[2]], (1, dimensions[2]))\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    # forward propagation\n",
    "    \n",
    "    a_1 = data\n",
    "    \n",
    "    z_2 = np.add(np.dot(a_1, W1), b1)\n",
    "    a_2 = sigmoid(z_2)\n",
    "    z_3 = np.add(np.dot(a_2, W2), b2)\n",
    "    a_3 = softmax(z_3)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    # back propagation\n",
    "    d_3 = np.subtract(a_3, labels)\n",
    "    \n",
    "    #cost calculation\n",
    "    i = 0\n",
    "    cost = 0\n",
    "    N = data.shape[0]\n",
    "    while i < N:\n",
    "        cost += np.dot(np.dot(-1, labels[i, :]), np.transpose(np.log(a_3[i, :])))\n",
    "        i += 1\n",
    "        \n",
    "    #back propagate the error to hidden layer\n",
    "    d_2 = np.multiply(np.dot(d_3, np.transpose(W2)), np.multiply(a_2, np.subtract(1, a_2)))\n",
    "    d_1 = np.multiply(np.dot(d_2, np.transpose(W1)), np.multiply(a_1, np.subtract(1, a_1)))\n",
    "    \n",
    "    gradb1 = np.zeros(b1.shape)\n",
    "    gradb2 = np.zeros(b2.shape)\n",
    "    \n",
    "    #calculate the gradients\n",
    "    gradW1 = np.dot(np.transpose(a_1), d_2)\n",
    "    gradW2 = np.dot(np.transpose(a_2), d_3)\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    while i < gradb1.shape[1]:\n",
    "        while j < N:\n",
    "            gradb1[0][i] += d_2[j][i]\n",
    "            j += 1\n",
    "        i += 1\n",
    "        j = 0\n",
    "    \n",
    "    i = 0\n",
    "    while i < gradb2.shape[1]:\n",
    "        while j < N:\n",
    "            gradb2[0][i] += d_3[j][i]\n",
    "            j += 1\n",
    "        i += 1\n",
    "        j = 0 \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Perform gradcheck on your neural network\n",
    "#print \"=== For autograder ===\"\n",
    "gradcheck_naive(lambda params: forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Implement a function that normalizes each row of a matrix to have unit length\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    sum = 0\n",
    "    \n",
    "    numRows = x.shape[0]\n",
    "    numColumns = x.shape[1]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    c = np.zeros((1, x.shape[0]))\n",
    "    while i < numRows:\n",
    "        while j < numColumns:\n",
    "            sum += np.square(x[i][j])\n",
    "            j += 1\n",
    "        c[0][i] = np.sqrt(sum)\n",
    "        i += 1\n",
    "        j = 0\n",
    "        sum = 0\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < numRows:\n",
    "        while j < numColumns:\n",
    "            x[i][j] = x[i][j] / c[0][i]\n",
    "            j += 1\n",
    "        i += 1\n",
    "        j = 0\n",
    "        \n",
    "            \n",
    " \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test this function\n",
    "print (\"=== For autograder ===\")\n",
    "print (normalizeRows(np.array([[3.0,4.0],[1, 2]])))  # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n",
    "\n",
    "# should produce\n",
    "# [[ 0.6         0.8       ]\n",
    "#  [ 0.4472136   0.89442719]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_wrapper(X, y, params, modelCostAndGradient, batchsize = 50):\n",
    "    C = X.shape[0]\n",
    "    \n",
    "    # randomly sample minibatch\n",
    "    if batchsize < C:\n",
    "        index = random.sample(range(0,C), batchsize)\n",
    "        X = X[index,:]\n",
    "        y = y[index,:]\n",
    "    \n",
    "    cost, grad = modelCostAndGradient(X, y, params)        \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, implement SGD\n",
    "\n",
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import os.path as op\n",
    "import _pickle as pickle\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "            \n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "    \n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the stochastic gradient descent method in this        #\n",
    "    # function.                                                       #\n",
    "    # Inputs:                                                         #\n",
    "    #   - f: the function to optimize, it should take a single        #\n",
    "    #        argument and yield two outputs, a cost and the gradient  #\n",
    "    #        with respect to the arguments                            #\n",
    "    #   - x0: the initial point to start SGD from                     #\n",
    "    #   - step: the step size for SGD                                 #\n",
    "    #   - iterations: total iterations to run SGD for                 #\n",
    "    #   - postprocessing: postprocessing function for the parameters  #\n",
    "    #        if necessary. In the case of word2vec we will need to    #\n",
    "    #        normalize the word vectors to have unit length.          #\n",
    "    #   - PRINT_EVERY: specifies every how many iterations to output  #\n",
    "    # Output:                                                         #\n",
    "    #   - x: the parameter value after SGD finishes                   #\n",
    "    ###################################################################\n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx;\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "            \n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "    \n",
    "    expcost = None\n",
    "    \n",
    "    for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        ### YOUR CODE HERE\n",
    "        ### Don't forget to apply the postprocessing after every iteration!\n",
    "        ### You might want to print the progress every few iterations.\n",
    "        \n",
    "        cost, grad = f(x)\n",
    "        x -= step*grad\n",
    "        x = postprocessing(x)\n",
    "        \n",
    "        \n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            print (\"iter:\\t\", iter, \"\\t cost:\\t\", cost)\n",
    "        \n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "            \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Use neural net to train hand-written digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets, neighbors, linear_model, cross_validation, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x218601dbc50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAGrCAYAAABg2IjeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAE5tJREFUeJzt3X+s5XV95/HXexxUwFon2mK7yrYj1rXBQB1A3S6yu85C\nYxRLsk5rSRNKaIPtGtLdTQqphmxjGtxGa9BiNptG/FHbwB+m6GpRpL/AshNGoSVCGx0VY4Hya4FA\nXbF89o9zyU4nVebce06/78v38UjuH+fMnXNegbn3eb/nfM+5NcYIAHSwY+oBAPAkUQKgDVECoA1R\nAqANUQKgDVECoA1RAqANUQKgjZ3rvPGqen6Ss5J8Lcm31nlfALT27CQ/kuTaMcb93+2T1hqlLIL0\ne2u+DwC2j3OTfOy7/eG6o/S1Nd8+R2Dv3r1rv49bbrklJ5988tpu/21ve9vabjtJ3vnOd+btb3/7\n2m5///79a7vtJLnqqquyb9++td7H5Zdfvtbbf+yxx3LMMces9T4eeeSRtd4+R+Rr3+sP1x0lD9k1\nsGvXrrXfx1FHHbXW+znxxBPXdttJ8tznPnet93H33Xev7baT5Oijj87xxx+/1vvYuXO93y527Nix\n9vughe/ZBSc6ANCGKAHQhigB0IYosRLrfj5j3d7whjdMPWFLTj311KknbNkzn/nMqSfQgCixEts9\nSmefffbUE7bktNNOm3rClokSiSgB0IgoAdCGKAHQhigB0IYoAdCGKAHQhigB0IYoAdCGKAHQhigB\n0MamolRVv1JVX62qv6+qm6pq+7/xFgCTWzpKVfUzSd6d5NIkP5Hk1iTXVtULVrwNgJnZzJHSryb5\nH2OMD48x7khyYZLHkpy/0mUAzM5SUaqqo5LsSfK5J68bY4wk1yV5zWqnATA3yx4pvSDJM5Lcc9j1\n9yR54UoWATBbzr4DoI1lo3Rfkn9Ictxh1x+X5O6VLAJgtpaK0hjj8SQHkrzuyeuqqjYuf3610wCY\nm52b+DvvSXJlVR1Isj+Ls/GOSXLlCncBMENLR2mMcdXGa5J+I4uH7W5JctYY495VjwNgXjZzpJQx\nxhVJrljxFgBmztl3ALQhSgC0IUoAtCFKALQhSgC0IUoAtCFKALQhSgC0IUoAtCFKALQhSgC0IUoA\ntCFKALQhSgC0IUoAtCFKALQhSgC0IUoAtCFKALSxc+oBrN9ll1029YQt271799QTtmTXrl1TT9iy\nBx54YOoJW7Zv376pJ2zJ1VdfPfWEtXOkBEAbogRAG6IEQBuiBEAbogRAG6IEQBuiBEAbogRAG6IE\nQBuiBEAbogRAG6IEQBuiBEAbogRAG6IEQBuiBEAbogRAG6IEQBtLR6mqTq+qa6rqm1X1RFWdvY5h\nAMzPZo6Ujk1yS5JfTjJWOweAOdu57F8YY/xRkj9KkqqqlS8CYLY8pwRAG6IEQBuiBEAbogRAG6IE\nQBtLn31XVccmOSHJk2fe7a6qk5I8MMb4xirHATAvS0cpySlJ/jiL1yiNJO/euP5DSc5f0S4AZmgz\nr1P603jYD4A1EBcA2hAlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDa\nECUA2hAlANoQJQDaECUA2hAlANoQJQDaWPrXoc/Rnj17pp6wJbt37556wpa95CUvmXrClhw8eHDq\nCVv22c9+duoJW7bdv5avvvrqqSesnSMlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDaECUA\n2hAlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDaECUA2lgqSlV1SVXtr6qHq+qeqvp4Vf3Y\nusYBMC/LHimdnuR9SV6VZG+So5J8pqqOXvUwAOZn5zKfPMZ4/aGXq+q8JH+XZE+SG1Y3C4A52upz\nSs9LMpI8sIItAMzcpqNUVZXkvUluGGN8aXWTAJirpR6+O8wVSX48yU+uaAsAM7epKFXV+5O8Psnp\nY4y7VjsJgLlaOkobQXpTkjPGGHeufhIAc7VUlKrqiiRvSXJ2kker6riNP3pojPGtVY8DYF6WPdHh\nwiTPTfInSf72kI99q50FwBwt+zolb0sEwNqIDABtiBIAbYgSAG2IEgBtiBIAbYgSAG2IEgBtiBIA\nbYgSAG2IEgBtiBIAbYgSAG2IEgBtiBIAbYgSAG2IEgBtiBIAbYgSAG2IEgBt7Jx6wHawa9euqSds\nyYEDB6aesGUHDx6cesLsPR3+HdGfIyUA2hAlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDa\nECUA2hAlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDaWCpKVXVhVd1aVQ9tfHy+qn5qXeMA\nmJdlj5S+keTXkrwyyZ4k1yf5w6p6+aqHATA/O5f55DHG/zrsqrdX1VuTvDrJ7StbBcAsLRWlQ1XV\njiT7khyT5C9WtgiA2Vo6SlV1YhYRenaSR5KcM8a4Y9XDAJifzZx9d0eSk5KcluQDST5cVf9qpasA\nmKWlj5TGGN9JcnDj4her6rQkFyV56yqHATA/q3id0o4kz1rB7QAwc0sdKVXVbyb5dJI7k3xfknOT\nnJHkzNVPA2Buln347geTfCjJDyV5KMlfJjlzjHH9qocBMD/Lvk7pgnUNAQDvfQdAG6IEQBuiBEAb\nogRAG6IEQBuiBEAbogRAG6IEQBuiBEAbogRAG6IEQBuiBEAbogRAG6IEQBuiBEAbogRAG6IEQBui\nBEAbS/069LnatWvX1BO25Lrrrpt6Ak8D2/3rIEkefPDBqSfwFBwpAdCGKAHQhigB0IYoAdCGKAHQ\nhigB0IYoAdCGKAHQhigB0IYoAdCGKAHQhigB0IYoAdCGKAHQhigB0IYoAdCGKAHQhigB0MaWolRV\nF1fVE1X1nlUNAmC+Nh2lqjo1yS8luXV1cwCYs01Fqaqek+SjSS5I8n9WugiA2drskdLvJPnEGOP6\nVY4BYN52LvsXqupnk5yc5JTVzwFgzpaKUlW9KMl7k+wdYzy+nkkAzNWyR0p7kvxAki9UVW1c94wk\nr62q/5TkWWOMscqBAMzHslG6LskrDrvuyiS3J7lMkADYiqWiNMZ4NMmXDr2uqh5Ncv8Y4/ZVDgNg\nflbxjg6OjgBYiaXPvjvcGOPfr2IIAHjvOwDaECUA2hAlANoQJQDaECUA2hAlANoQJQDaECUA2hAl\nANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDa2Dn1gO3gwQcfnHrCluzZ\ns2fqCbO3a9euqSds2dPh39HVV1899QSegiMlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDa\nECUA2hAlANoQJQDaECUA2hAlANoQJQDaECUA2hAlANoQJQDaWCpKVXVpVT1x2MeX1jUOgHnZzK9D\nvy3J65LUxuXvrG4OAHO2mSh9Z4xx78qXADB7m3lO6aVV9c2q+kpVfbSqXrzyVQDM0rJRuinJeUnO\nSnJhkh9N8mdVdeyKdwEwQ0s9fDfGuPaQi7dV1f4kX0+yL8kHVzkMgPnZ0inhY4yHkvxNkhNWMweA\nOdtSlKrqOVkE6a7VzAFgzpZ9ndJvVdVrq+pfVtW/TvLxJI8n+f21rANgVpY9JfxFST6W5PlJ7k1y\nQ5JXjzHuX/UwAOZn2RMd3rKuIQDgve8AaEOUAGhDlABoQ5QAaEOUAGhDlABoQ5QAaEOUAGhDlABo\nQ5QAaEOUAGhDlABoQ5QAaEOUAGhDlABoQ5QAaEOUAGhDlABoQ5QAaGPn1AO2g4MHD049YUv27Nkz\n9YQte/Ob3zz1hC3Z7vufLt71rndNPYGn4EgJgDZECYA2RAmANkQJgDZECYA2RAmANkQJgDZECYA2\nRAmANkQJgDZECYA2RAmANkQJgDZECYA2RAmANkQJgDZECYA2RAmANpaOUlX9cFV9pKruq6rHqurW\nqnrlOsYBMC87l/nkqnpekhuTfC7JWUnuS/LSJA+ufhoAc7NUlJJcnOTOMcYFh1z39RXuAWDGln34\n7o1Jbq6qq6rqnqr6QlVd8JR/CwCOwLJR2p3krUn+OsmZST6Q5PKq+vlVDwNgfpZ9+G5Hkv1jjHds\nXL61qk5McmGSj6x0GQCzs+yR0l1Jbj/sutuTHL+aOQDM2bJRujHJyw677mVxsgMAK7BslH47yaur\n6pKqeklV/VySC5K8f/XTAJibpaI0xrg5yTlJ3pLkr5L8epKLxhh/sIZtAMzMsic6ZIzxqSSfWsMW\nAGbOe98B0IYoAdCGKAHQhigB0IYoAdCGKAHQhigB0IYoAdCGKAHQhigB0IYoAdCGKAHQhigB0IYo\nAdCGKAHQhigB0IYoAdCGKAHQhigB0MbOqQdsBwcPHpx6wpZcfPHFU0/Ysssuu2zqCVty4MCBqSds\n2SmnnDL1BGbAkRIAbYgSAG2IEgBtiBIAbYgSAG2IEgBtiBIAbYgSAG2IEgBtiBIAbYgSAG2IEgBt\niBIAbYgSAG2IEgBtiBIAbYgSAG0sFaWq+mpVPfFPfLxvXQMBmI9lfx36KUmeccjlVyT5TJKrVrYI\ngNlaKkpjjPsPvVxVb0zylTHGn690FQCztOnnlKrqqCTnJvnd1c0BYM62cqLDOUm+P8mHVrQFgJnb\nSpTOT/LpMcbdqxoDwLwte6JDkqSqjk+yN8lPr3YOAHO22SOl85Pck+RTK9wCwMwtHaWqqiTnJbly\njPHEyhcBMFubOVLam+TFST644i0AzNzSzymNMT6bf/wCWgBYCe99B0AbogRAG6IEQBuiBEAbogRA\nG6IEQBuiBEAbogRAG6IEQBuiBEAbogRAG6IEQBuiBEAbosRKfPnLX556wpZcc801U0/YkhtuuGHq\nCbASosRKbPcoffKTn5x6wpbceOONU0+AlRAlANoQJQDaECUA2lj616Ev6dlrvn2OwH333bf2+/j2\nt7+91vu57bbb1nbbSfLwww+v9T4OHjy4tttOkscee2zt9wEr8j27UGOMtd1zVf1ckt9b2x0AsN2c\nO8b42Hf7w3VH6flJzkrytSTfWtsdAdDds5P8SJJrxxj3f7dPWmuUAGAZTnQAoA1RAqANUQKgDVEC\noI1tHaWq+pWq+mpV/X1V3VRVp0696UhV1elVdU1VfbOqnqiqs6fetIyquqSq9lfVw1V1T1V9vKp+\nbOpdy6iqC6vq1qp6aOPj81X1U1Pv2qyqunjj39J7pt5yJKrq0o29h358aepdy6qqH66qj1TVfVX1\n2Ma/qVdOvetIbHz/PPz/wRNV9b6pNm3bKFXVzyR5d5JLk/xEkluTXFtVL5h02JE7NsktSX45yXY8\nBfL0JO9L8qoke5McleQzVXX0pKuW840kv5bklUn2JLk+yR9W1csnXbUJGz+Q/VIWXwfbyW1Jjkvy\nwo2PfzPtnOVU1fOS3Jjk/2bx8peXJ/kvSR6cctcSTsn//2//wiT/IYvvR1dNNWjbnhJeVTcl+d9j\njIs2LlcW32QuH2P890nHLamqnkjy02OMbfv7EzZ+GPi7JK8dY2zb36NQVfcn+a9jjA9OveVIVdVz\nkhxI8tYk70jyxTHGf5521VOrqkuTvGmMsS2OKv4pVXVZkteMMc6YessqVNV7k7x+jDHZox7b8kip\nqo7K4ifbzz153VjU9bokr5lq18w9L4ufsB6YeshmVNWOqvrZJMck+Yup9yzpd5J8Yoxx/dRDNuGl\nGw9hf6WqPlpVL5560JLemOTmqrpq42HsL1TVBVOP2oyN76vnJvndKXdsyygleUGSZyS557Dr78ni\nEJR/RhtHqe9NcsMYY1s9J1BVJ1bVI1k8/HJFknPGGHdMPOuIbYT05CSXTL1lE25Kcl4WD3tdmORH\nk/xZVR075agl7c7iCPWvk5yZ5ANJLq+qn5901eack+T7k3xoyhHrfkNW5uGKJD+e5CenHrIJdyQ5\nKYsvxv+Y5MNV9drtEKaqelEWPwzsHWM8PvWeZY0xrj3k4m1VtT/J15PsS7JdHj7dkWT/GOMdG5dv\nraoTs4jsR6abtSnnJ/n0GOPuKUds1yOl+5L8QxZPkB7quCST/gedm6p6f5LXJ/m3Y4y7pt6zrDHG\nd8YYB8cYXxxj/HoWJwpcNPWuI7QnyQ8k+UJVPV5Vjyc5I8lFVfXtjSPYbWOM8VCSv0lywtRblnBX\nktsPu+72JMdPsGXTqur4LE5Y+p9Tb9mWUdr4qfBAktc9ed3GF+Drknx+ql1zsxGkNyX5d2OMO6fe\nsyI7kjxr6hFH6Lokr8ji4buTNj5uTvLRJCeNbXYW08YJGydk8Y1+u7gxycsOu+5lWRzxbSfnZ/H0\nx6emHrKdH757T5Irq+pAkv1JfjWLJ6mvnHLUkdp43PyEJE/+NLu7qk5K8sAY4xvTLTsyVXVFkrck\nOTvJo1X15FHrQ2OMbfGO8FX1m0k+neTOJN+XxZO8Z2Tx3EB7Y4xHk/yj5/Cq6tEk948xDv/pvZ2q\n+q0kn8jiG/i/SPLfkjye5Pen3LWk305yY1VdksVp1K9KckGSX5x01RI2fqA/L8mVY4wnJp6zfaM0\nxrhq4zTk38jiYbtbkpw1xrh32mVH7JQkf5zFGWsji9dcJYsnGc+fatQSLsxi958cdv0vJPnwP/ua\nzfnBLP57/1CSh5L8ZZIzt+lZbE/aTkdHL0rysSTPT3JvkhuSvPp7/VqDbsYYN1fVOUkuy+J0/K8m\nuWiM8QfTLlvK3iQvTpPn8bbt65QAePrZls8pAfD0JEoAtCFKALQhSgC0IUoAtCFKALQhSgC0IUoA\ntCFKALQhSgC0IUoAtCFKALTx/wA50jBDFP8cOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2186017b080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(digits.images[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the targets: labeled as 0,1,...,9\n",
    "digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits_X = digits.images.reshape(digits.images.shape[0],-1)/15\n",
    "N = len(digits.target)\n",
    "digits_y = np.zeros((N,10))\n",
    "for i in xrange(N):\n",
    "    digits_y[i,digits.target[i]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into training and testing, only use traning data for learning\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\\\n",
    "                            digits_X, digits_y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specificy architecture of the neural network, do not modify this code\n",
    "# And initalize parameters\n",
    "dimensions = [digits_X.shape[1], 20, 10]\n",
    "np.random.seed(1)\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters for running sgd, do not modify\n",
    "\n",
    "step = 0.01\n",
    "iterations = 50000\n",
    "batchsize = 50\n",
    "\n",
    "# call sgd and sgd_wrapper to learn parameters of the neural network defined above using training data\n",
    "\n",
    "### YOUR CODE HERE\n",
    "index = 0\n",
    "while (index < iterations):\n",
    "    cost, grad = sgd_wrapper(X_train, y_train, params, forward_backward_prop)\n",
    "    params = np.subtract(params, np.multiply(step, grad))\n",
    "    index += 1\n",
    "\n",
    "learned_params = params\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763]\n"
     ]
    }
   ],
   "source": [
    "# check your learned parameters\n",
    "print (learned_params[0:5])\n",
    "\n",
    "# shoud produce\n",
    "# [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check prediction on test data\n",
    "yy = nn_predict(X_test, learned_params, dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666203059805285"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction accuracy on test\n",
    "metrics.accuracy_score(yy.argmax(axis=1), y_test.argmax(axis=1))\n",
    "\n",
    "# should produce 0.96105702364394996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
